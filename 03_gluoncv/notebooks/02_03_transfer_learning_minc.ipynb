{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Transfer Learning with Your Own Image Dataset\n=======================================================\n\nDataset size is a big factor in the performance of deep learning models.\n``ImageNet`` has over one million labeled images, but\nwe often don't have so much labeled data in other domains.\nTraining a deep learning models on small datasets may lead to severe overfitting.\n\nTransfer learning is a technique that addresses this problem.\nThe idea is simple: we can start training with a pre-trained model,\ninstead of starting from scratch.\nAs Isaac Newton said, \"If I have seen further it is by standing on the\nshoulders of Giants\".\n\nIn this tutorial, we will explain the basics of transfer\nlearning, and apply it to the ``MINC-2500`` dataset.\n\nData Preparation\n----------------\n\n`MINC <http://opensurfaces.cs.cornell.edu/publications/minc/>`__ is\nshort for Materials in Context Database, provided by Cornell.\n``MINC-2500`` is a resized subset of ``MINC`` with 23 classes, and 2500\nimages in each class. It is well labeled and has a moderate size thus is\nperfect to be our example.\n\n|image-minc|\n\nTo start, we first download ``MINC-2500`` from\n`here <http://opensurfaces.cs.cornell.edu/publications/minc/>`__.\nSuppose we have the data downloaded to ``~/data/`` and\nextracted to ``~/data/minc-2500``.\n\nAfter extraction, it occupies around 2.6GB disk space with the following\nstructure:\n\n::\n\n    minc-2500\n    \u251c\u2500\u2500 README.txt\n    \u251c\u2500\u2500 categories.txt\n    \u251c\u2500\u2500 images\n    \u2514\u2500\u2500 labels\n\nThe ``images`` folder has 23 sub-folders for 23 classes, and ``labels``\nfolder contains five different splits for training, validation, and test.\n\nWe have written a script to prepare the data for you:\n\n:download:`Download prepare_minc.py<../../../scripts/classification/finetune/prepare_minc.py>`\n\nRun it with\n\n::\n\n    python prepare_minc.py --data ~/data/minc-2500 --split 1\n\nNow we have the following structure:\n\n::\n\n    minc-2500\n    \u251c\u2500\u2500 categories.txt\n    \u251c\u2500\u2500 images\n    \u251c\u2500\u2500 labels\n    \u251c\u2500\u2500 README.txt\n    \u251c\u2500\u2500 test\n    \u251c\u2500\u2500 train\n    \u2514\u2500\u2500 val\n\nIn order to go through this tutorial within a reasonable amount of time,\nwe have prepared a small subset of the ``MINC-2500`` dataset,\nbut you should substitute it with the original dataset for your experiments.\nWe can download and extract it with:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import zipfile, os\nfrom gluoncv.utils import download\n\nfile_url = 'https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/classification/minc-2500-tiny.zip'\nzip_file = download(file_url, path='./')\nwith zipfile.ZipFile(zip_file, 'r') as zin:\n    zin.extractall(os.path.expanduser('./'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparameters\n----------\n\nFirst, let's import all other necessary libraries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mxnet as mx\nimport numpy as np\nimport os, time, shutil\n\nfrom mxnet import gluon, image, init, nd\nfrom mxnet import autograd as ag\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.model_zoo import vision as models\nfrom mxnet.gluon.data.vision import transforms\nfrom gluoncv.utils import makedirs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We set the hyperparameters as following:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "classes = 23\n\nepochs = 5\nlr = 0.001\nper_device_batch_size = 1\nmomentum = 0.9\nwd = 0.0001\n\nlr_factor = 0.75\nlr_steps = [10, 20, 30, np.inf]\n\nnum_gpus = 1\nnum_workers = 8\nctx = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\nbatch_size = per_device_batch_size * max(num_gpus, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Things to keep in mind:\n\n1. ``epochs = 5`` is just for this tutorial with the tiny dataset. please change it to a larger number in your experiments, for instance 40.\n2. ``per_device_batch_size`` is also set to a small number. In your experiments you can try larger number like 64.\n3. remember to tune ``num_gpus`` and ``num_workers`` according to your machine.\n4. A pre-trained model is already in a pretty good status. So we can start with a small ``lr``.\n\nData Augmentation\n-----------------\n\nIn transfer learning, data augmentation can also help.\nWe use the following augmentation in training:\n\n2. Randomly crop the image and resize it to 224x224\n3. Randomly flip the image horizontally\n4. Randomly jitter color and add noise\n5. Transpose the data from height*width*num_channels to num_channels*height*width, and map values from [0, 255] to [0, 1]\n6. Normalize with the mean and standard deviation from the ImageNet dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "jitter_param = 0.4\nlighting_param = 0.1\n\ntransform_train = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomFlipLeftRight(),\n    transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n                                 saturation=jitter_param),\n    transforms.RandomLighting(lighting_param),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the data augmentation functions, we can define our data loaders:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "path = './minc-2500-tiny'\ntrain_path = os.path.join(path, 'train')\nval_path = os.path.join(path, 'val')\ntest_path = os.path.join(path, 'test')\n\ntrain_data = gluon.data.DataLoader(\n    gluon.data.vision.ImageFolderDataset(train_path).transform_first(transform_train),\n    batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\nval_data = gluon.data.DataLoader(\n    gluon.data.vision.ImageFolderDataset(val_path).transform_first(transform_test),\n    batch_size=batch_size, shuffle=False, num_workers = num_workers)\n\ntest_data = gluon.data.DataLoader(\n    gluon.data.vision.ImageFolderDataset(test_path).transform_first(transform_test),\n    batch_size=batch_size, shuffle=False, num_workers = num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that only ``train_data`` uses ``transform_train``, while\n``val_data`` and ``test_data`` use ``transform_test`` to produce deterministic\nresults for evaluation.\n\nModel and Trainer\n-----------------\n\nWe use a pre-trained ``ResNet50_v2`` model, which has balanced accuracy and\ncomputation cost.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_name = 'ResNet50_v2'\nfinetune_net = gluon.model_zoo.vision.get_model(model_name, pretrained=True)\nwith finetune_net.name_scope():\n    finetune_net.output = nn.Dense(classes)\nfinetune_net.output.initialize(init.Xavier(), ctx = ctx)\nfinetune_net.collect_params().reset_ctx(ctx)\nfinetune_net.hybridize()\n\ntrainer = gluon.Trainer(finetune_net.collect_params(), 'sgd', {\n                        'learning_rate': lr, 'momentum': momentum, 'wd': wd})\nmetric = mx.metric.Accuracy()\nL = gluon.loss.SoftmaxCrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's an illustration of the pre-trained model\nand our newly defined model:\n\n|image-model|\n\nSpecifically, we define the new model by::\n\n1. load the pre-trained model\n2. re-define the output layer for the new task\n3. train the network\n\nThis is called \"fine-tuning\", i.e. we have a model trained on another task,\nand we would like to tune it for the dataset we have in hand.\n\nWe define a evaluation function for validation and testing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test(net, val_data, ctx):\n    metric = mx.metric.Accuracy()\n    for i, batch in enumerate(val_data):\n        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n        outputs = [net(X) for X in data]\n        metric.update(label, outputs)\n\n    return metric.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Loop\n-------------\n\nFollowing is the main training loop. It is the same as the loop in\n`CIFAR10 <dive_deep_cifar10.html>`__\nand ImageNet.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Once again, in order to go through the tutorial faster, we are training on a small\n    subset of the original ``MINC-2500`` dataset, and for only 5 epochs. By training on the\n    full dataset with 40 epochs, it is expected to get accuracy around 80% on test data.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr_counter = 0\nnum_batch = len(train_data)\n\nfor epoch in range(epochs):\n    if epoch == lr_steps[lr_counter]:\n        trainer.set_learning_rate(trainer.learning_rate*lr_factor)\n        lr_counter += 1\n\n    tic = time.time()\n    train_loss = 0\n    metric.reset()\n\n    for i, batch in enumerate(train_data):\n        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n        with ag.record():\n            outputs = [finetune_net(X) for X in data]\n            loss = [L(yhat, y) for yhat, y in zip(outputs, label)]\n        for l in loss:\n            l.backward()\n\n        trainer.step(batch_size)\n        train_loss += sum([l.mean().asscalar() for l in loss]) / len(loss)\n\n        metric.update(label, outputs)\n\n    _, train_acc = metric.get()\n    train_loss /= num_batch\n\n    _, val_acc = test(finetune_net, val_data, ctx)\n\n    print('[Epoch %d] Train-acc: %.3f, loss: %.3f | Val-acc: %.3f | time: %.1f' %\n             (epoch, train_acc, train_loss, val_acc, time.time() - tic))\n\n_, test_acc = test(finetune_net, test_data, ctx)\nprint('[Finished] Test-acc: %.3f' % (test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next\n----\n\nNow that you have learned to muster the power of transfer\nlearning, to learn more about training a model on\nImageNet, please read `this tutorial <dive_deep_imagenet.html>`__.\n\nThe idea of transfer learning is the basis of\n`object detection <../examples_detection/index.html>`_ and\n`semantic segmentation <../examples_segmentation/index.html>`_,\nthe next two chapters of our tutorial.\n\n.. |image-minc| image:: https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/datasets/MINC-2500.png\n.. |image-model| image:: https://zh.gluon.ai/_images/fine-tuning.svg\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}